{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJtFEvyGUaS1bkDrJK9QGJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with RNN\n",
        "\n",
        "In this notebook, we will implement an RNN in Tensorflow for generating text at character-level.\n",
        "\n",
        "**Note:** This notebook has been created as part of the Encoder-Decoder Architecture course on Google Cloud Skills Boost platform."
      ],
      "metadata": {
        "id": "XJAeoMSfGEPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "lHAFtxyUGykS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are setting up the libraries and reading the dataset."
      ],
      "metadata": {
        "id": "lrPPErfyKEX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "R3_bpQc7G3fI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lCVxe5IoGA4O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "4rnGiuJiG-Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        ")"
      ],
      "metadata": {
        "id": "rKYK0L2tG_vb",
        "outputId": "3570c93d-e520-44e2-b696-e12eca0677be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Length of text: {len(text)} characters.\")"
      ],
      "metadata": {
        "id": "rjjmvwxYHIxi",
        "outputId": "b0c600a7-e2b3-4864-c38e-eb1a23093d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "id": "vXUGmjTjJnYk",
        "outputId": "5e58d44f-c684-4fe2-b535-e56b63928ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters.\")"
      ],
      "metadata": {
        "id": "rK9QutD-JpPY",
        "outputId": "227cde70-2d5f-4710-e616-3ba59473c8a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "KbPagZ1fJ6Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will process the text to get it in a format that can be used for training an RNN Encoder-Decoder."
      ],
      "metadata": {
        "id": "51v8L5MoJ9It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Mapping\n",
        "Mapping characters to ids."
      ],
      "metadata": {
        "id": "i68VrlD3KRiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "FZs1yQSKJ4cA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reverse Mapping\n",
        "Mapping ids to characters."
      ],
      "metadata": {
        "id": "A2zL1iKqKW_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "3t_AwxDYKNKg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We are using `ids_from_chars.get_vocabulary()` instead of passing the original vocabulary `vocab` for inverse mapping so that `[UNK]` token gets set too."
      ],
      "metadata": {
        "id": "SGf57jZAKjZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing a utility function to return as one string a list of ids."
      ],
      "metadata": {
        "id": "ETTi9b1yLD_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids: list) -> list:\n",
        "    \"\"\"\n",
        "        Function to return as single strings a list of list of ids.\n",
        "\n",
        "        Arguments:\n",
        "            ids (list): List of list of ids.\n",
        "\n",
        "        Returns (list): Returns a list of strings reverse mapped from the ids.\n",
        "    \"\"\"\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "BagVP6BVK1Nd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training examples and targets."
      ],
      "metadata": {
        "id": "dcRXQ552Dc0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "V6vcIfAbDcep"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "qqYsV6sbDtaw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(text_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDskybEmOTDH",
        "outputId": "2a6a5beb-3ab4-4b4f-c6f8-5d3bf43f7c2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "Task: Given a character, or a sequence of characters, what is the most probable next character?"
      ],
      "metadata": {
        "id": "i1dhqBcdDRlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence: list) -> tuple:\n",
        "    \"\"\"\n",
        "        Function that returns input and target (input shifted by 1) from a\n",
        "        sequence.\n",
        "\n",
        "        Arguments:\n",
        "            sequence (list): Input sequence\n",
        "\n",
        "        Returns (tuple): (list, list) of input and target\n",
        "    \"\"\"\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "z1e5NNGvLw1i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "RXpzKnTNPGkG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "R6aUjdZmPMLF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model"
      ],
      "metadata": {
        "id": "QQmZ7XlcPn74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "lg7I8cIIPm0-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "2wCGSvbyPue4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "N-RcxJkJQUe3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8_6ywXOQwQj",
        "outputId": "7f80e453-b821-4e72-868d-1977b235f4eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQxeDa6gQ-sv",
        "outputId": "c34d7172-49e4-4a38-83cb-832143f6517d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "VR24Mu2sRDWX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "\n",
        "print(\"Mean loss: \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es3WQ727RLDc",
        "outputId": "665691d5-caa2-4c3c-ba5c-2f5bae0d40ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:  tf.Tensor(4.1898823, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nup4XpX_RWZL",
        "outputId": "a9d66f0f-a10f-402e-ac3d-38e5418295e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.01502"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "xqkdxITFRb4Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix, save_weights_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "WaFPCvZQRkU4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "xLzVr1DWRwGP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUyAW-9nRyIi",
        "outputId": "5c456fce-cffe-4525-a47e-50173afcfd6a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 14s 59ms/step - loss: 2.7217\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.9888\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.7067\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.5456\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4476\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3804\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3279\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2836\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2412\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 16s 61ms/step - loss: 1.2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text"
      ],
      "metadata": {
        "id": "bW1s3ixBSPRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None] # To skip UNK from generation\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            values = [-float(\"inf\")] * len(skip_ids),\n",
        "            indices = skip_ids,\n",
        "            dense_shape = [len(ids_from_chars.get_vocabulary())]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "\n",
        "        # Only use last prediction\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        predicted_logits = predicted_logits + self.prediction_mask # Apply UNK mask\n",
        "\n",
        "        # Sample output logits to generate token IDs\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return characters and model state\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "jrMN6HdZR1pX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "2W2lCob5URNZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()"
      ],
      "metadata": {
        "id": "Fo9icqicUUSL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDsUarYkVksz",
        "outputId": "6b99dd11-8175-481c-8862-0992217d17f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I for, like pains; heaven want thee stay,\n",
            "His robable traitors unwasted o'er the court\n",
            "When he's a charm a holy a should\n",
            "With witden mean rubject\n",
            "Of fitles wish, that swear I mean,\n",
            "think you dit? wratch, may it is, I'll go not helice,\n",
            "Let me encures and yet encounter'd. I hear you\n",
            "Your loss\n",
            "And begrar here from the fear,\n",
            "My life in the pricker doub, I know you then first fence I fled.\n",
            "He has the Henry'cle of the sun atternhy\n",
            "The emperion in our bebals;\n",
            "And underneat' to see where they come both proclaim'st\n",
            "This friend upon the petter,\n",
            "And therefore I'll be full of you of it;\n",
            "indeed, I'ld auntay; sus his hand me one of Gloucestory!\n",
            "'What weak a heapt of my desiish father in\n",
            "his neelf, so than where I live: I cannot\n",
            "Believe not to all meat, worthy Murderer, an hours but sad\n",
            "A harder no have and countinal.\n",
            "\n",
            "KING RICHARD III:\n",
            "We remember me well deliver'd here to ride.\n",
            "\n",
            "ROMEO:\n",
            "I meant, and a knight of ignor house of Gloucester,\n",
            "Do most issure fill, they can do guess;\n",
            "Yet, make it my roofi \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2629642486572266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7cIFwAHVqoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}